---
layout: post
title: Ollama 사용 방법
description: >
  Ollama 사용 방법에 대해 설명하는 페이지입니다.
image:
  path: /assets/img/raspberry-pi/ollama/ollama.png
related_posts:
  - None
sitemap: true
comments: false
---

<i>last modified at: 2025. 01. 30.</i>

<i>Environment</i>

- <i>Device: Raspberry Pi 4 Model B</i>
- <i>OS: Raspberry Pi OS (64 bit)</i>
- <i>RAM: 4GB</i>

<h2>목차</h2>

- [개요](#개요)
- [Ollama란?](#ollama란)
  - [개념](#개념)
  - [Ollama의 특징](#ollama의-특징)
  - [지원 모델](#지원-모델)
- [Step 1 - Ollama 설치하기](#step-1---ollama-설치하기)
  - [Linux에 설치하기](#linux에-설치하기)
  - [Docker로 설치하기](#docker로-설치하기)
- [Step 2 - Ollama 실행하기](#step-2---ollama-실행하기)
  - [Linux에서 실행하기](#linux에서-실행하기)
  - [Docker로 실행하기](#docker로-실행하기)
- [Step 3 - Ollama 명령어 사용하기](#step-3---ollama-명령어-사용하기)
  - [Ollama 실행 - ollama serve](#ollama-실행---ollama-serve)
  - [LLM 다운로드 - ollama pull](#llm-다운로드---ollama-pull)
  - [다운로드한 LLM 목록 확인 - ollama list](#다운로드한-llm-목록-확인---ollama-list)
  - [LLM 정보 확인 - ollama show](#llm-정보-확인---ollama-show)
  - [LLM 실행 - ollama run](#llm-실행---ollama-run)
  - [실행 중인 LLM 목록 확인 - ollama ps](#실행-중인-llm-목록-확인---ollama-ps)
  - [실행 중인 LLM 중단 - ollama stop](#실행-중인-llm-중단---ollama-stop)
- [Step 4 - LLM 다운로드하기](#step-4---llm-다운로드하기)
- [Step 5 - LLM 실행하기](#step-5---llm-실행하기)
- [Step 6 - REST API 사용하기](#step-6---rest-api-사용하기)
- [참고 자료](#참고-자료)
- [Comments](#comments)

## 개요

이번 글에서는 Raspberry Pi에 Ollama를 설치하고 사용하는 방법에 대해 설명하겠습니다.

## Ollama란?

### 개념

`Ollama`란 `대규모 언어 모델(Large Language Model, LLM)`을 로컬 환경에서 쉽게 실행하고 관리할 수 있도록 설계된 오픈소스 플랫폼 및 도구입니다. Ollama는 Windows, macOS, Linux 같은 다양한 운영체제에서 설치 및 실행할 수 있으며, Llama, Gemma, Mistral, DeepSeek 등 다양한 LLM을 지원합니다. 또한 REST API로 다른 애플리케이션과의 연동도 지원합니다.

### Ollama의 특징

Ollama의 특징은 다음과 같습니다.

- `로컬 실행`

  인터넷 연결 없이 로컬 환경에서 LLM을 사용할 수 있습니다.

- `다양한 모델 지원`

  Ollama는 Llama, Gemma, Mistral, DeepSeek 등 다양한 LLM을 지원합니다.

- `간단한 설치 및 사용`

  CLI를 통해 쉽게 설치 및 실행할 수 있습니다.

- `REST API 제공`

  Ollama는 REST API를 제공하여 LLM을 다른 애플리케이션과 쉽게 연동할 수 있도록 지원합니다.

- `모델 커스터마이징 지원`

  `GGUF(Georgi Gerganov Unified Format)` 형태의 모델을 임포트하거나 사용자 지정 모델을 학습시킬 수 있습니다.

### 지원 모델

Ollama는 다양한 LLM을 지원합니다. 주요 LLM은 다음과 같습니다.

| Model              | Parameters | Size  | Download                         |
| ------------------ | ---------- | ----- | -------------------------------- |
| DeekSeek-R1        | 671B       | 404GB | `ollama run deepseek-r1:671b`    |
| DeekSeek-R1        | 70B        | 43GB  | `ollama run deepseek-r1:70b`     |
| DeekSeek-R1        | 32B        | 20GB  | `ollama run deepseek-r1:32b`     |
| DeekSeek-R1        | 14B        | 9.0GB | `ollama run deepseek-r1:14b`     |
| DeekSeek-R1        | 8B         | 4.8GB | `ollama run deepseek-r1:8b`      |
| DeekSeek-R1        | 7B         | 4.7GB | `ollama run deepseek-r1:7b`      |
| DeekSeek-R1        | 1.5B       | 1.1GB | `ollama run deepseek-r1:1.5b`    |
| Llama 3.3          | 70B        | 43GB  | `ollama run llama3.3`            |
| Llama 3.2          | 3B         | 2.0GB | `ollama run llama3.2`            |
| Llama 3.2          | 1B         | 1.3GB | `ollama run llama3.2:1b`         |
| Llama 3.2 Vision   | 11B        | 7.9GB | `ollama run llama3.2-vision`     |
| Llama 3.2 Vision   | 90B        | 55GB  | `ollama run llama3.2-vision:90b` |
| Llama 3.1          | 8B         | 4.7GB | `ollama run llama3.1`            |
| Llama 3.1          | 405B       | 231GB | `ollama run llama3.1:405b`       |
| Phi 4              | 14B        | 9.1GB | `ollama run phi4`                |
| Phi 3 Mini         | 3.8B       | 2.3GB | `ollama run phi3`                |
| Gemma 2            | 2B         | 1.6GB | `ollama run gemma2:2b`           |
| Gemma 2            | 9B         | 5.5GB | `ollama run gemma2`              |
| Gemma 2            | 27B        | 16GB  | `ollama run gemma2:27b`          |
| Mistral            | 7B         | 4.1GB | `ollama run mistral`             |
| Moondream 2        | 1.4B       | 829MB | `ollama run moondream`           |
| Neural Chat        | 7B         | 4.1GB | `ollama run neural-chat`         |
| Starling           | 7B         | 4.1GB | `ollama run starling-lm`         |
| Code Llama         | 7B         | 3.8GB | `ollama run codellama`           |
| Llama 2 Uncensored | 7B         | 3.8GB | `ollama run llama2-uncensored`   |
| LLaVA              | 7B         | 4.5GB | `ollama run llava`               |
| Solar              | 10.7B      | 6.1GB | `ollama run solar`               |

Ollama는 위의 표에 적힌 LLM 이외에도 다양한 LLM을 지원합니다. 사용할 수 있는 전체 LLM 목록은 <a href="https://ollama.com/search" target="_blank">https://ollama.com/search</a>에서 확인하실 수 있습니다.

## Step 1 - Ollama 설치하기

`Raspberry Pi OS (64 bit)`는 Linux 운영체제에 해당하므로 다음 2가지 방식으로 Ollama를 설치할 수 있습니다. 이번 글에서는 Docker로 설치하여 사용하는 방식을 선택하였습니다.

### Linux에 설치하기

다음 명령어를 입력하여 Linux에 Ollama를 설치할 수 있습니다. 설치 중 NVIDIA/AMD GPU가 자동으로 감지되며, 해당 GPU가 없는 경우 CPU 모드로 동작합니다.

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### Docker로 설치하기

Docker로 Ollama를 사용하기 위해선 먼저 Docker를 설치해야 합니다. Docker 설치 방법은 [Docker의 개념과 사용 방법](/raspberry-pi/2024-11-06-docker/) 문서를 참고하시길 바랍니다. Docker가 설치되어 있다면 다음 명령어를 입력하여 Docker Hub에서 Ollama 공식 이미지를 다운로드할 수 있습니다.

```bash
docker pull ollama/ollama
```

<img src="/assets/img/raspberry-pi/ollama/pic1.png" alt="pic1" style="box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19); border-radius: 0.5rem"/>

## Step 2 - Ollama 실행하기

### Linux에서 실행하기

다음 명령어를 입력하여 Ollama를 실행할 수 있습니다.

```bash
ollama serve
```

### Docker로 실행하기

다음 명령어를 입력하여 Docker 이미지를 통해 Ollama를 실행할 수 있습니다. 해당 명령어는 `CPU`만 사용하는 방식입니다. GPU를 사용하는 방식은 다음 링크를 참고하시길 바랍니다.

<a href="https://hub.docker.com/r/ollama/ollama" target="_blank">https://hub.docker.com/r/ollama/ollama</a>

```bash
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

## Step 3 - Ollama 명령어 사용하기

이 문단에서는 주요 ollama 명령어에 대해 설명합니다. Docker로 ollama를 사용하는 경우 각 명령어 앞에 `docker exec -it ollama`를 추가로 작성하거나 (Ex. `docker exec -it ollama` ollama list) 또는 `docker exec -it ollama bash` 명령어를 먼저 입력하여 interactive shell을 사용하면 됩니다.

### Ollama 실행 - ollama serve

`ollama serve` 명령어를 입력하여 ollama를 실행할 수 있습니다. Docker로 Ollama를 설치한 경우 `docker run` 명령어 실행 시점에 자동으로 실행됩니다.

```bash
ollama serve
```

### LLM 다운로드 - ollama pull

`ollama pull [모델명]` 명령어를 입력하여 LLM을 다운로드할 수 있습니다. 또한 해당 명령어는 로컬 모델을 업데이트할 때도 사용될 수 있습니다.

```bash
ollama pull llama3.2
```

<img src="/assets/img/raspberry-pi/ollama/pic2.png" alt="pic2" style="box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19); border-radius: 0.5rem"/>

### 다운로드한 LLM 목록 확인 - ollama list

`ollama list` 명령어를 입력하여 로컬에 다운로드한 LLM 목록을 확인할 수 있습니다.

```bash
ollama list
```

<img src="/assets/img/raspberry-pi/ollama/pic3.png" alt="pic3" style="box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19); border-radius: 0.5rem"/>

### LLM 정보 확인 - ollama show

`ollama show [모델명]` 명령어를 입력하여 로컬에 다운로드한 특정 LLM 정보를 확인할 수 있습니다.

```bash
ollama show llama3.2
```

<img src="/assets/img/raspberry-pi/ollama/pic4.png" alt="pic4" style="box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19); border-radius: 0.5rem"/>

### LLM 실행 - ollama run

`ollama run [모델명]` 명령어를 입력하여 LLM을 실행할 수 있습니다. 만약 로컬에 실행하고자 하는 LLM이 없는 경우 자동으로 다운로드합니다.

```bash
ollama run llama3.2
```

<img src="/assets/img/raspberry-pi/ollama/pic5.png" alt="pic5" style="box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19); border-radius: 0.5rem"/>

### 실행 중인 LLM 목록 확인 - ollama ps

`ollama ps` 명령어를 입력하여 현재 실행 중인 LLM 목록을 확인할 수 있습니다.

```bash
ollama ps
```

<img src="/assets/img/raspberry-pi/ollama/pic6.png" alt="pic6" style="box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19); border-radius: 0.5rem"/>

### 실행 중인 LLM 중단 - ollama stop

`ollama stop [모델명]` 명령어를 입력하여 현재 실행 중인 LLM을 중단시킬 수 있습니다.

```bash
ollama stop llama3.2
```

<img src="/assets/img/raspberry-pi/ollama/pic7.png" alt="pic7" style="box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19); border-radius: 0.5rem"/>

## Step 4 - LLM 다운로드하기

Ollama를 실행한 후 LLM을 사용하기 위해선 먼저 LLM를 다운로드해야 합니다. 이번 글에서는 `Llama 3.2(3B, 2.0GM)`를 다운로드해서 실행하겠습니다. 다음 명령어를 입력하여 `Llama 3.2`을 다운로드합니다.

```bash
ollama pull llama3.2
```

<img src="/assets/img/raspberry-pi/ollama/pic2.png" alt="pic2" style="box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19); border-radius: 0.5rem"/>

다운로드한 LLM 목록을 확인하기 위해 다음 명령어를 입력합니다.

```bash
ollama list
```

<img src="/assets/img/raspberry-pi/ollama/pic3.png" alt="pic3" style="box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19); border-radius: 0.5rem"/>

## Step 5 - LLM 실행하기

LLM을 실행하기 위해 다음 명령어를 입력합니다.

```bash
ollama run llama3.2
```

LLM을 실행하면 다음과 같이 CLI에서 직접 모델과 채팅 형식으로 상호작용할 수 있습니다.

<img src="/assets/img/raspberry-pi/ollama/pic5.png" alt="pic5" style="box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19); border-radius: 0.5rem"/>

## Step 6 - REST API 사용하기

Ollama는 REST API를 제공하여 LLM을 다른 애플리케이션과 쉽게 연동할 수 있도록 지원합니다. 예를 들어 LLM과 대화를 하기 위해선 다음과 같이 호출할 수 있습니다.

<img src="/assets/img/raspberry-pi/ollama/pic8.png" alt="pic8" style="box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19); border-radius: 0.5rem"/>

위에서 설명한 API 외의 다른 API도 지원합니다. 전체 API 목록은 다음 링크에서 확인하실 수 있습니다.

<a href="https://github.com/ollama/ollama/blob/main/docs/api.md" target="_blank">https://github.com/ollama/ollama/blob/main/docs/api.md</a>

또한 위에서 사용한 방식이 아니라 library를 설치해서 API를 호출할 수도 있습니다.

- <a href="https://github.com/ollama/ollama-js" target="_blank">https://github.com/ollama/ollama-js</a>
- <a href="https://github.com/ollama/ollama-python" target="_blank">https://github.com/ollama/ollama-python</a>

## 참고 자료

- <a href="https://github.com/ollama/ollama" target="_blank">https://github.com/ollama/ollama</a>
- <a href="https://ollama.com/" target="_blank">https://ollama.com/</a>
- <a href="https://hub.docker.com/r/ollama/ollama" target="_blank">https://hub.docker.com/r/ollama/ollama</a>

## Comments

<hr />
<script
  src="https://utteranc.es/client.js"
  repo="HyunJinNo/HyunJinNo.github.io"
  issue-term="pathname"
  theme="github-light"
  crossorigin="anonymous"
  async
></script>
